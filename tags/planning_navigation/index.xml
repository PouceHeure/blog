<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Planning_navigation on Hugo POUSSEUR</title><link>https://pouceheure.github.io/blog/tags/planning_navigation/</link><description>Recent content in Planning_navigation on Hugo POUSSEUR</description><generator>Hugo</generator><language>en-en</language><lastBuildDate>Tue, 10 Dec 2024 00:00:00 +0000</lastBuildDate><atom:link href="https://pouceheure.github.io/blog/tags/planning_navigation/index.xml" rel="self" type="application/rss+xml"/><item><title>Autonomous Vehicle Planning</title><link>https://pouceheure.github.io/blog/projects/project_autosys_local-planning/</link><pubDate>Tue, 10 Dec 2024 00:00:00 +0000</pubDate><guid>https://pouceheure.github.io/blog/projects/project_autosys_local-planning/</guid><description>&lt;h2 id="demonstration"&gt;Demonstration&lt;/h2&gt;






&lt;figure &gt;
 &lt;div class="ratio ratio-16x9" style="max-width: 800px; margin: auto;"&gt;
 &lt;iframe src="https://www.youtube.com/embed/wnQpdtmPgv0" allowfullscreen&gt;&lt;/iframe&gt;
 &lt;/div&gt;
 &lt;figcaption&gt;Video 1 - Demo video, slalom test.&lt;/figcaption&gt;
&lt;/figure&gt;














&lt;figure &gt;
 &lt;div class="ratio ratio-16x9" style="max-width: 800px; margin: auto;"&gt;
 &lt;iframe src="https://www.youtube.com/embed/As44QMtXXiw" allowfullscreen&gt;&lt;/iframe&gt;
 &lt;/div&gt;
 &lt;figcaption&gt;Video 2 - Demo video, curve test.&lt;/figcaption&gt;
&lt;/figure&gt;








&lt;h2 id="overview"&gt;Overview&lt;/h2&gt;
&lt;p&gt;As part of a collaborative autonomous driving project, I contributed to the implementation of the core planning functionalities for autonomous driving.&lt;br&gt;
The navigation stack transforms a target position into a safe and efficient motion trajectory using environmental context and map information.&lt;/p&gt;
&lt;p&gt;The process is divided into three stages:&lt;/p&gt;</description></item><item><title>Thesis: Shared navigation in a cybernetic multi-agent autonomous system</title><link>https://pouceheure.github.io/blog/articles/article_thesis/</link><pubDate>Thu, 28 Mar 2024 00:00:00 +0000</pubDate><guid>https://pouceheure.github.io/blog/articles/article_thesis/</guid><description>&lt;h2 id="abstract"&gt;Abstract&lt;/h2&gt;
&lt;p&gt;My thesis is based on shared navigation between the autonomous system and the human. In our research, we focus on command fusion. In our approach, both entities, the human and the autonomous system, simultaneously control the vehicle, and a module acquires their commands and performs the command fusion. This approach involves studying the intentions of both the human and the autonomous system to ensure the most appropriate fusion of their choices and to evaluate the decision-making of each entity. The intention of the autonomous system is calculated using a visual servoing controller. The implementation of visual servoing relies on a deep learning network detecting lanes. For the human driver, who actively drives and cannot express their intention simultaneously, we use a deep learning-based model to predict their intention. The construction of this model required the creation of a driving dataset using our vehicles and the development of a recurrent model that integrates data of various types. Each of these intentions is then evaluated according to specific criteria, including safety, comfort, and context, to guide the fusion process towards the selection of the highest quality intention. This quantification is based on a state analysis derived from the realization of these intentions. We then use game theory to facilitate the fusion process, where each entity, human and autonomous system, aims to steer the final command towards their choice.&lt;/p&gt;</description></item><item><title>Driving Intention Quantification Formula</title><link>https://pouceheure.github.io/blog/projects/project_intention-quantification/</link><pubDate>Sun, 10 Dec 2023 00:00:00 +0000</pubDate><guid>https://pouceheure.github.io/blog/projects/project_intention-quantification/</guid><description>&lt;h2 id="abstract"&gt;Abstract&lt;/h2&gt;
&lt;p&gt;As part of a collaborative research effort on shared control between an autonomous driving system and a human driver, I contributed to work focused on evaluating the &lt;strong&gt;quality&lt;/strong&gt; of driving intentions.&lt;br&gt;
A &lt;strong&gt;driving intention&lt;/strong&gt; is defined as a short sequence of planned control actions (speed, steering) over a time horizon:&lt;/p&gt;


&lt;blockquote&gt;
&lt;div class="equation-scroll-wrapper"&gt;
 
 &lt;div class="equation-block"&gt;
 $$ 
I_t = \{(v,w)_{t+0\cdot\Delta t},(v,w)_{t+1\cdot\Delta t},...,(v,w)_{t+n\cdot\Delta t}\}
 \quad \text{Eq (1)} $$
 &lt;/div&gt;
 
&lt;/div&gt;
&lt;/blockquote&gt;

&lt;p&gt;For example, an intention can be expressed as a sequence of linear and angular velocities applied to the vehicle.&lt;/p&gt;</description></item><item><title>Cooperative Architecture Using Air and Ground Vehicles for the Search and Recognition of Targets</title><link>https://pouceheure.github.io/blog/articles/article_cooperative-architecture/</link><pubDate>Sun, 01 Jan 2023 00:00:00 +0000</pubDate><guid>https://pouceheure.github.io/blog/articles/article_cooperative-architecture/</guid><description>&lt;h2 id="abstract"&gt;Abstract&lt;/h2&gt;
&lt;p&gt;A cooperative navigation architecture for the search and recognition of targets using aerial and ground vehicles is proposed in this paper. The architecture allows managing aerial and ground vehicles to autonomously perform different tasks independently or cooperatively. For our application, two main tasks are conceived: aerial monitoring of a surface to search for targets and target ground recognition. In the target aerial detection task, the aerial drone autonomously follows a trajectory computed to cover the entire surface to be monitored, searching for targets using vision algorithms. Once a target is detected, its relative position is sent to the cooperative architecture. After the aerial drone has covered the entire area, the architecture computes and assigns each ground vehicle the closest target found. Each ground vehicle then navigates autonomously, avoiding obstacles if present, to its assigned target. To verify the success of the mission, the aerial vehicle flies following the dynamic center of mass of the ground vehicles. Real-time experiments are carried out to validate the proposed architecture. The main results, depicted in some graphs, corroborate the good performance in a closed loop.&lt;/p&gt;</description></item><item><title>Dynamic Context Awareness in Autonomous Navigation</title><link>https://pouceheure.github.io/blog/articles/article_context-awareness/</link><pubDate>Sat, 01 Jan 2022 00:00:00 +0000</pubDate><guid>https://pouceheure.github.io/blog/articles/article_context-awareness/</guid><description>&lt;h2 id="abstract"&gt;Abstract&lt;/h2&gt;
&lt;p&gt;Many studies faced the problem of vehicle autonomous navigation in different fields, but nowadays just a few of them uses all the implicit information coming from the context in which such navigation is occurring. This results in a huge potential information loss that prevents us from adapting the vehicle’s behavior to each different situation it may be in. In a previous work, we defined a method to model the static context of navigation using ontologies and take it into account in the command law when performing a local navigation task. In this paper, we extend our model of the context of navigation, and define a software architecture able to update the context dynamically, by using sensor information. The method is tested with real-time experiments on driving simulator. They show that the Context of Navigation can be effectively updated during the navigation and leads to a smarter vehicle’s behavior on the road.&lt;/p&gt;</description></item><item><title>Gradient Descent Dynamic Window Approach to Mobile Robot Autonomous Navigation</title><link>https://pouceheure.github.io/blog/articles/article_gradient-descent/</link><pubDate>Sat, 01 Jan 2022 00:00:00 +0000</pubDate><guid>https://pouceheure.github.io/blog/articles/article_gradient-descent/</guid><description>&lt;h2 id="abstract"&gt;Abstract&lt;/h2&gt;
&lt;p&gt;Avoiding obstacles is a key feature in a vehicle autonomous navigation methodology. The Dynamic Window Approach (DWA), proposed several decades ago, has emerged as a responsive navigation methodology suitable for reactive obstacle avoidance. In the initial approach of the DWA, the optimization of an objective function is realized with an exhaustive computation, which can be costly in computational time. This is not useful in a real-time scenario where an autonomous vehicle needs to avoid obstacles in urban or road velocity conditions. In this paper, we revise the DWA methodology by implementing a new method that redefines the objective function as a loss function, allowing the application of gradient descent for optimization. We verified the correctness of our optimization by controlling a robot in an unknown environment with obstacles to visit given positions. Our approach was tested in simulation on ROS and on a real Turtlebot robot, demonstrating improved runtime execution and a less abrupt, more comfortable driving experience.&lt;/p&gt;</description></item><item><title>Optimization of the Dynamic Window Approach (DWA)</title><link>https://pouceheure.github.io/blog/projects/project_dwa-optimisation/</link><pubDate>Fri, 10 Dec 2021 00:00:00 +0000</pubDate><guid>https://pouceheure.github.io/blog/projects/project_dwa-optimisation/</guid><description>&lt;h2 id="overview"&gt;Overview&lt;/h2&gt;
&lt;p&gt;The Dynamic Window Approach (DWA) is a reactive motion planning method used in mobile robotics. At each control cycle, it samples admissible velocity pairs $(v, \omega)$ and evaluates them using an objective function, selecting the pair with the highest score.&lt;/p&gt;
&lt;p&gt;The original method presents two main issues:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Discrete sampling&lt;/strong&gt;, which limits precision&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;High computational cost&lt;/strong&gt; in evaluation loops&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;These limitations can be addressed by defining a &lt;strong&gt;convex, differentiable objective function&lt;/strong&gt; and applying &lt;strong&gt;gradient descent&lt;/strong&gt; for continuous optimization.&lt;/p&gt;</description></item><item><title>Context Modelling Applied to the Intelligent Vehicle Navigation</title><link>https://pouceheure.github.io/blog/articles/article_context-modelling/</link><pubDate>Fri, 01 Jan 2021 00:00:00 +0000</pubDate><guid>https://pouceheure.github.io/blog/articles/article_context-modelling/</guid><description>&lt;h2 id="abstract"&gt;Abstract&lt;/h2&gt;
&lt;p&gt;This paper faces the problem of intelligent vehicles in interaction with their occupants and the environment, by modelling the semantic context associated with navigation. By semantically modelling context, an intelligent vehicle can not only drive itself safely but also reason about the situation and act accordingly. To achieve this, it is necessary to first define the Context of Navigation and then establish inference rules to enrich the vehicle&amp;rsquo;s understanding of the situation. We propose a definition of the Context of Navigation, based on information pertinent to the vehicle&amp;rsquo;s controller, dividing it into two components: the Dynamic Context and the Static Context. This paper focuses on the latter.&lt;/p&gt;</description></item><item><title>Shared Decision-Making Forward an Autonomous Navigation for Intelligent Vehicles</title><link>https://pouceheure.github.io/blog/articles/article_shared-decision/</link><pubDate>Fri, 01 Jan 2021 00:00:00 +0000</pubDate><guid>https://pouceheure.github.io/blog/articles/article_shared-decision/</guid><description>&lt;h2 id="abstract"&gt;Abstract&lt;/h2&gt;
&lt;p&gt;In this paper, a non-haptic shared control is applied to navigate an intelligent vehicle based on two inputs: one by a human driver and the other by an autonomous system. The proposed shared decision-making model aims to improve the safety and efficiency of autonomous vehicle navigation by integrating human intuition and machine precision. Experimental results demonstrate the effectiveness of this approach in various driving scenarios. :contentReference[oaicite:0]{index=0}&lt;/p&gt;</description></item></channel></rss>