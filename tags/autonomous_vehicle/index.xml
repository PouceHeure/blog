<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Autonomous_vehicle on Hugo POUSSEUR</title><link>https://pouceheure.github.io/blog/tags/autonomous_vehicle/</link><description>Recent content in Autonomous_vehicle on Hugo POUSSEUR</description><generator>Hugo</generator><language>en-en</language><lastBuildDate>Tue, 14 Oct 2025 00:00:00 +0000</lastBuildDate><atom:link href="https://pouceheure.github.io/blog/tags/autonomous_vehicle/index.xml" rel="self" type="application/rss+xml"/><item><title>LiDAR Detection &amp; Making Decision</title><link>https://pouceheure.github.io/blog/projects/project_autosys_lidar_and_decision/</link><pubDate>Tue, 14 Oct 2025 00:00:00 +0000</pubDate><guid>https://pouceheure.github.io/blog/projects/project_autosys_lidar_and_decision/</guid><description>&lt;h2 id="demonstration"&gt;Demonstration&lt;/h2&gt;






&lt;figure id="video_demo_lidar"&gt;
 &lt;div class="ratio ratio-16x9" style="max-width: 800px; margin: auto;"&gt;
 &lt;iframe src="https://www.youtube.com/embed/YJY-9dpiC7o" allowfullscreen&gt;&lt;/iframe&gt;
 &lt;/div&gt;
 &lt;figcaption&gt;Video 1 - Video demo of the LiDAR Detection pipeline&lt;/figcaption&gt;
&lt;/figure&gt;




 





&lt;blockquote&gt;
&lt;p&gt;Info: A video demonstration of the decision-making system is coming soon.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id="introduction"&gt;Introduction&lt;/h2&gt;
&lt;p&gt;In autonomous driving, the vehicle must be able to make a series of decisions, such as slowing down, stopping at an intersection, or waiting until the road is clear before proceeding.&lt;/p&gt;
&lt;p&gt;These decisions depend on the behavior of other road users. The system must therefore identify surrounding obstacles and determine which ones are relevant for decision-making.&lt;/p&gt;</description></item><item><title>Traffic Light Detection &amp; Control</title><link>https://pouceheure.github.io/blog/projects/project_autosys_traffic-light-detection/</link><pubDate>Thu, 15 May 2025 00:00:00 +0000</pubDate><guid>https://pouceheure.github.io/blog/projects/project_autosys_traffic-light-detection/</guid><description>&lt;h2 id="motivation"&gt;Motivation&lt;/h2&gt;
&lt;p&gt;As part of an autonomous driving project, I contributed to the development of a system enabling a vehicle to adapt its behavior to traffic lights. The vehicle adjusts its speed according to the current traffic light state.&lt;/p&gt;
&lt;p&gt;The project is divided into four main components:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Matching&lt;/strong&gt;: Identifying traffic light positions on the map&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Detection&lt;/strong&gt;: Determining the current state of the traffic light&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Planning&lt;/strong&gt;: Adjusting vehicle speed according to the detection result&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Control&lt;/strong&gt;: Applying the computed velocity to the vehicle&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The detection pipeline is implemented in Python, while control is handled in C++ within a ROS2 environment.&lt;/p&gt;</description></item><item><title>Autonomous Vehicle: Control</title><link>https://pouceheure.github.io/blog/projects/project_autosys_control/</link><pubDate>Fri, 20 Dec 2024 00:00:00 +0000</pubDate><guid>https://pouceheure.github.io/blog/projects/project_autosys_control/</guid><description>&lt;h2 id="demonstration"&gt;Demonstration&lt;/h2&gt;






&lt;figure &gt;
 &lt;div class="ratio ratio-16x9" style="max-width: 800px; margin: auto;"&gt;
 &lt;iframe src="https://www.youtube.com/embed/nJx0B9U1x-g" allowfullscreen&gt;&lt;/iframe&gt;
 &lt;/div&gt;
 &lt;figcaption&gt;Video 1 - Demo video: planning and control in action.&lt;/figcaption&gt;
&lt;/figure&gt;








&lt;h2 id="overview"&gt;Overview&lt;/h2&gt;
&lt;p&gt;As part of a collaborative autonomous driving project, I contributed to the &lt;strong&gt;low-level control&lt;/strong&gt; system responsible for translating navigation commands into actuator signals (steering and torque).&lt;br&gt;
This system is fully integrated into a ROS2-based autonomous stack and has been deployed and tested on a &lt;strong&gt;real electric vehicle with in-wheel motors&lt;/strong&gt;.&lt;/p&gt;
&lt;figure id="fig-1"&gt;
 &lt;img src="https://pouceheure.github.io/blog/images/autosys-control/control_interfaces_inputs.png" alt=""
 width="500"
 &gt;
 &lt;figcaption&gt;Fig. 1 - Interfaces between planning and control modules.&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;h2 id="control-structure"&gt;Control Structure&lt;/h2&gt;
&lt;p&gt;The controller is composed of two main components:&lt;/p&gt;</description></item><item><title>Autonomous Vehicle: Planning</title><link>https://pouceheure.github.io/blog/projects/project_autosys_local-planning/</link><pubDate>Tue, 10 Dec 2024 00:00:00 +0000</pubDate><guid>https://pouceheure.github.io/blog/projects/project_autosys_local-planning/</guid><description>&lt;h2 id="demonstration"&gt;Demonstration&lt;/h2&gt;






&lt;figure &gt;
 &lt;div class="ratio ratio-16x9" style="max-width: 800px; margin: auto;"&gt;
 &lt;iframe src="https://www.youtube.com/embed/wnQpdtmPgv0" allowfullscreen&gt;&lt;/iframe&gt;
 &lt;/div&gt;
 &lt;figcaption&gt;Video 1 - Demo video, slalom test.&lt;/figcaption&gt;
&lt;/figure&gt;














&lt;figure &gt;
 &lt;div class="ratio ratio-16x9" style="max-width: 800px; margin: auto;"&gt;
 &lt;iframe src="https://www.youtube.com/embed/As44QMtXXiw" allowfullscreen&gt;&lt;/iframe&gt;
 &lt;/div&gt;
 &lt;figcaption&gt;Video 2 - Demo video, curve test.&lt;/figcaption&gt;
&lt;/figure&gt;








&lt;h2 id="overview"&gt;Overview&lt;/h2&gt;
&lt;p&gt;As part of a collaborative autonomous driving project, I contributed to the implementation of the core planning functionalities for autonomous driving.&lt;br&gt;
The navigation stack transforms a target position into a safe and efficient motion trajectory using environmental context and map information.&lt;/p&gt;
&lt;p&gt;The process is divided into three stages:&lt;/p&gt;</description></item><item><title>Thesis: Shared navigation in a cybernetic multi-agent autonomous system</title><link>https://pouceheure.github.io/blog/articles/article_thesis/</link><pubDate>Thu, 28 Mar 2024 00:00:00 +0000</pubDate><guid>https://pouceheure.github.io/blog/articles/article_thesis/</guid><description>&lt;h2 id="abstract"&gt;Abstract&lt;/h2&gt;
&lt;p&gt;My thesis is based on shared navigation between the autonomous system and the human. In our research, we focus on command fusion. In our approach, both entities, the human and the autonomous system, simultaneously control the vehicle, and a module acquires their commands and performs the command fusion. This approach involves studying the intentions of both the human and the autonomous system to ensure the most appropriate fusion of their choices and to evaluate the decision-making of each entity. The intention of the autonomous system is calculated using a visual servoing controller. The implementation of visual servoing relies on a deep learning network detecting lanes. For the human driver, who actively drives and cannot express their intention simultaneously, we use a deep learning-based model to predict their intention. The construction of this model required the creation of a driving dataset using our vehicles and the development of a recurrent model that integrates data of various types. Each of these intentions is then evaluated according to specific criteria, including safety, comfort, and context, to guide the fusion process towards the selection of the highest quality intention. This quantification is based on a state analysis derived from the realization of these intentions. We then use game theory to facilitate the fusion process, where each entity, human and autonomous system, aims to steer the final command towards their choice.&lt;/p&gt;</description></item><item><title>Visual Control Applied To Autonomous Vehicle</title><link>https://pouceheure.github.io/blog/projects/project_visual-control/</link><pubDate>Wed, 01 Feb 2023 00:00:00 +0000</pubDate><guid>https://pouceheure.github.io/blog/projects/project_visual-control/</guid><description>&lt;h2 id="abstract"&gt;Abstract&lt;/h2&gt;
&lt;p&gt;As part of a research project, I contributed to the development and testing of a &lt;strong&gt;Visual Servoing&lt;/strong&gt; framework, a control strategy that links image feature variations to robot velocities. Applied to autonomous vehicles, this approach enables lane-keeping purely from camera input, without GPS or map dependency. It integrates lane detection using deep learning and transforms the output into features used in a control law that centers the vehicle in the lane.&lt;/p&gt;</description></item><item><title>General and Multi-Criteria Approach to Study the Admissibility and Quality of a Driving Intention</title><link>https://pouceheure.github.io/blog/articles/article_general-multi-criteria/</link><pubDate>Sun, 01 Jan 2023 00:00:00 +0000</pubDate><guid>https://pouceheure.github.io/blog/articles/article_general-multi-criteria/</guid><description>&lt;h2 id="abstract"&gt;Abstract&lt;/h2&gt;
&lt;p&gt;Determining the admissibility and quality of driving intentions, generated by an automated intelligent vehicle and by a human driver, is a sine qua none task in a human-intelligent vehicle share navigation. Our paper proposes a generic method to quantify driving intentions. These intentions are defined by a sequence of velocities. This formulation is based on metrics already discussed in the literature. It proposes a way to use them to evaluate a state, making a judgment, and to extend this evaluation to a sequence of states. This quantification determines whether the intention is safely achievable and defines a quality taking into account several criteria (safety, comfort, context, energy consumption). It is thus possible to compare and rank the intentions according to a set of criteria. This article defines a proposed implementation that has been tested on a driving simulator. For a given scenario, we tested our solution on several intentions in order to show the interest of our solution, and the possibility to compare the intentions between them.&lt;/p&gt;</description></item><item><title>Lane Detection</title><link>https://pouceheure.github.io/blog/projects/project_lane-detection/</link><pubDate>Mon, 10 Jan 2022 00:00:00 +0000</pubDate><guid>https://pouceheure.github.io/blog/projects/project_lane-detection/</guid><description>&lt;h2 id="abstract"&gt;Abstract&lt;/h2&gt;
&lt;p&gt;Lane detection is an essential function for autonomous vehicles. While GPS and HD maps offer accurate localization under optimal conditions, vision-based detection provides an important backup, especially in areas with weak GNSS signals or outdated maps.&lt;/p&gt;
&lt;p&gt;This system implements a deep-learning lane detection pipeline using a convolutional autoencoder, designed to identify and segment multiple lane boundaries in various road conditions using only front-facing camera images.&lt;/p&gt;
&lt;p&gt;The approach integrates image preprocessing, deep-learning inference, optional temporal tracking, and curve fitting to generate stable and clean lane boundaries. It has been tested in real-world driving and in simulators such as Carla and Scaner.&lt;/p&gt;</description></item><item><title>Prediction of human driving behavior using deep learning: a recurrent learning structure</title><link>https://pouceheure.github.io/blog/articles/article_human-driving-prediction/</link><pubDate>Sat, 01 Jan 2022 00:00:00 +0000</pubDate><guid>https://pouceheure.github.io/blog/articles/article_human-driving-prediction/</guid><description>&lt;h2 id="abstract"&gt;Abstract&lt;/h2&gt;
&lt;p&gt;Predicting the intentions of the human and the machine on a near future is required to the human-machine shared control of automated intelligent vehicles. The autonomous system is able to inform about its future intentions, however it is not possible for the human to provide this information, it is necessary then to predict it. This paper proposes a deep learning methodology to predict human navigation intentions in a time horizon of a few seconds, using a recurrent neural network (RNN) architecture based on the Long Short-Term Memory (LSTM) architecture. Taking as input various preprocessed and non-preprocessed data, generated by embedded sensors and the intrinsic data of the vehicle, the proposed model predicts the future linear and angular velocities of the vehicle. The model was trained and tested on a dataset created from real data from our cars equipped with sensors (LiDAR, camera), in different scenarios and road types. Furthermore, a data sensitive study is presented evaluating the effects of missing data in the learning process.&lt;/p&gt;</description></item><item><title>Shared Decision-Making Forward an Autonomous Navigation for Intelligent Vehicles</title><link>https://pouceheure.github.io/blog/articles/article_shared-decision/</link><pubDate>Fri, 01 Jan 2021 00:00:00 +0000</pubDate><guid>https://pouceheure.github.io/blog/articles/article_shared-decision/</guid><description>&lt;h2 id="abstract"&gt;Abstract&lt;/h2&gt;
&lt;p&gt;In this paper, a non-haptic shared control is applied to navigate an intelligent vehicle based on two inputs: one by a human driver and the other by an autonomous system. The proposed shared decision-making model aims to improve the safety and efficiency of autonomous vehicle navigation by integrating human intuition and machine precision. Experimental results demonstrate the effectiveness of this approach in various driving scenarios. :contentReference[oaicite:0]{index=0}&lt;/p&gt;</description></item></channel></rss>