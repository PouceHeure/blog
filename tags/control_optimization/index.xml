<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Control_optimization on Hugo POUSSEUR</title><link>https://pouceheure.github.io/blog/tags/control_optimization/</link><description>Recent content in Control_optimization on Hugo POUSSEUR</description><generator>Hugo</generator><language>en-en</language><lastBuildDate>Mon, 10 Nov 2025 00:00:00 +0000</lastBuildDate><atom:link href="https://pouceheure.github.io/blog/tags/control_optimization/index.xml" rel="self" type="application/rss+xml"/><item><title>Autonomous Vehicle: Teleop</title><link>https://pouceheure.github.io/blog/projects/project_autosys_teleop/</link><pubDate>Mon, 10 Nov 2025 00:00:00 +0000</pubDate><guid>https://pouceheure.github.io/blog/projects/project_autosys_teleop/</guid><description>&lt;h2 id="demonstration"&gt;Demonstration&lt;/h2&gt;
&lt;h3 id="video"&gt;Video&lt;/h3&gt;
&lt;p&gt;





&lt;figure id="video-demo-teleop"&gt;
 &lt;div class="ratio ratio-16x9" style="max-width: 800px; margin: auto;"&gt;
 &lt;iframe src="https://www.youtube.com/embed/wK4yOg2SyBs" allowfullscreen&gt;&lt;/iframe&gt;
 &lt;/div&gt;
 &lt;figcaption&gt;Video 1 - Teleoperation Demo. (Train Demo)&lt;/figcaption&gt;
&lt;/figure&gt;




 




 
 

This 

 &lt;a href="#video-demo-teleop"&gt;Video 1: Teleoperation Demo. (Train Demo)&lt;/a&gt;

 illustrates the longitudinal control of the car via teleoperation.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Example of speed saturation: 0:07 - 0:16 (limited by lateral acceleration)&lt;/li&gt;
&lt;li&gt;Example of breaking: 1:20 (vehicle is stopped by teleop command)&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="context"&gt;Context&lt;/h3&gt;
&lt;p&gt;The vehicle autonomously plans a path and follows it.&lt;br&gt;
A teleoperator controls only the longitudinal speed using a joystick, similar to driving a train. The operator specifies the desired speed &lt;em&gt;along the pre-defined path&lt;/em&gt;, but does not steer.&lt;/p&gt;</description></item><item><title>Traffic Light Detection &amp; Control</title><link>https://pouceheure.github.io/blog/projects/project_autosys_traffic-light-detection/</link><pubDate>Thu, 15 May 2025 00:00:00 +0000</pubDate><guid>https://pouceheure.github.io/blog/projects/project_autosys_traffic-light-detection/</guid><description>&lt;h2 id="motivation"&gt;Motivation&lt;/h2&gt;
&lt;p&gt;As part of an autonomous driving project, I contributed to the development of a system enabling a vehicle to adapt its behavior to traffic lights. The vehicle adjusts its speed according to the current traffic light state.&lt;/p&gt;
&lt;p&gt;The project is divided into four main components:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Matching&lt;/strong&gt;: Identifying traffic light positions on the map&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Detection&lt;/strong&gt;: Determining the current state of the traffic light&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Planning&lt;/strong&gt;: Adjusting vehicle speed according to the detection result&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Control&lt;/strong&gt;: Applying the computed velocity to the vehicle&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The detection pipeline is implemented in Python, while control is handled in C++ within a ROS2 environment.&lt;/p&gt;</description></item><item><title>Autonomous Vehicle: Control</title><link>https://pouceheure.github.io/blog/projects/project_autosys_control/</link><pubDate>Fri, 20 Dec 2024 00:00:00 +0000</pubDate><guid>https://pouceheure.github.io/blog/projects/project_autosys_control/</guid><description>&lt;h2 id="demonstration"&gt;Demonstration&lt;/h2&gt;






&lt;figure &gt;
 &lt;div class="ratio ratio-16x9" style="max-width: 800px; margin: auto;"&gt;
 &lt;iframe src="https://www.youtube.com/embed/nJx0B9U1x-g" allowfullscreen&gt;&lt;/iframe&gt;
 &lt;/div&gt;
 &lt;figcaption&gt;Video 1 - Demo video: planning and control in action.&lt;/figcaption&gt;
&lt;/figure&gt;







 
 

&lt;h2 id="overview"&gt;Overview&lt;/h2&gt;
&lt;p&gt;As part of a collaborative autonomous driving project, I contributed to the &lt;strong&gt;low-level control&lt;/strong&gt; system responsible for translating navigation commands into actuator signals (steering and torque).&lt;br&gt;
This system is fully integrated into a ROS2-based autonomous stack and has been deployed and tested on a &lt;strong&gt;real electric vehicle with in-wheel motors&lt;/strong&gt;.&lt;/p&gt;
&lt;figure id="fig-1"&gt;
 &lt;img src="https://pouceheure.github.io/blog/images/autosys-control/control_interfaces_inputs.png" alt=""
 width="500"
 &gt;
 &lt;figcaption&gt;Fig. 1 - Interfaces between planning and control modules.&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;h2 id="control-structure"&gt;Control Structure&lt;/h2&gt;
&lt;p&gt;The controller is composed of two main components:&lt;/p&gt;</description></item><item><title>Visual Control Applied To Autonomous Vehicle</title><link>https://pouceheure.github.io/blog/projects/project_visual-control/</link><pubDate>Wed, 01 Feb 2023 00:00:00 +0000</pubDate><guid>https://pouceheure.github.io/blog/projects/project_visual-control/</guid><description>&lt;h2 id="abstract"&gt;Abstract&lt;/h2&gt;
&lt;p&gt;As part of a research project, I contributed to the development and testing of a &lt;strong&gt;Visual Servoing&lt;/strong&gt; framework, a control strategy that links image feature variations to robot velocities. Applied to autonomous vehicles, this approach enables lane-keeping purely from camera input, without GPS or map dependency. It integrates lane detection using deep learning and transforms the output into features used in a control law that centers the vehicle in the lane.&lt;/p&gt;</description></item><item><title>Model-based and Machine Learning-based High-level Controller for Autonomous Vehicle Navigation: Lane Centering and Obstacles Avoidance</title><link>https://pouceheure.github.io/blog/articles/article_machine-learning-controller/</link><pubDate>Sun, 01 Jan 2023 00:00:00 +0000</pubDate><guid>https://pouceheure.github.io/blog/articles/article_machine-learning-controller/</guid><description>&lt;h2 id="abstract"&gt;Abstract&lt;/h2&gt;
&lt;p&gt;Researchers have been attempting to make the car drive autonomously. The environment perception together with safe guidance and control is an important task and are one of the big challenges when developing this kind of system. Geometrical or physical based models, machine learning based models and those based on a mixture of both models, are the three types of navigation methods used to resolve this problem. The last method takes advantage of the learning capability of machine learning models and uses the safeness of geometric models in order to better perform the navigation task. This paper presents a hybrid autonomous navigation methodology, which takes advantage of the learning capability of machine learning and uses the safeness of the dynamic window approach geometric method. Using a single camera and a 2D lidar sensor, this method actuates as a high-level controller, where optimal vehicle velocities are found, then applied by a low-level controller. The final algorithm is validated on CARLA Simulator environment, where the system proved to be capable to guide the vehicle in order to achieve the following tasks: lane keeping and obstacle avoidance.&lt;/p&gt;</description></item><item><title>Proposal of On-board Camera-Based Driving Force Control Method for Autonomous Electric Vehicles</title><link>https://pouceheure.github.io/blog/articles/article_camera-based-control/</link><pubDate>Sun, 01 Jan 2023 00:00:00 +0000</pubDate><guid>https://pouceheure.github.io/blog/articles/article_camera-based-control/</guid><description>&lt;h2 id="abstract"&gt;Abstract&lt;/h2&gt;
&lt;p&gt;By utilizing the camera installed in the front of a vehicle, this paper proposes on-board camera-based driving force control (CDFC) methods for autonomous electric vehicles driven by in-wheel motors. The image processing algorithm can detect the change in road surface conditions quickly and accurately. This enables the CDFC to update the slip-ratio limiter in real-time. Test results show that the proposed methods can improve traction control performance and reduce inverter input energy.&lt;/p&gt;</description></item><item><title>Gradient Descent Dynamic Window Approach to Mobile Robot Autonomous Navigation</title><link>https://pouceheure.github.io/blog/articles/article_gradient-descent/</link><pubDate>Sat, 01 Jan 2022 00:00:00 +0000</pubDate><guid>https://pouceheure.github.io/blog/articles/article_gradient-descent/</guid><description>&lt;h2 id="abstract"&gt;Abstract&lt;/h2&gt;
&lt;p&gt;Avoiding obstacles is a key feature in a vehicle autonomous navigation methodology. The Dynamic Window Approach (DWA), proposed several decades ago, has emerged as a responsive navigation methodology suitable for reactive obstacle avoidance. In the initial approach of the DWA, the optimization of an objective function is realized with an exhaustive computation, which can be costly in computational time. This is not useful in a real-time scenario where an autonomous vehicle needs to avoid obstacles in urban or road velocity conditions. In this paper, we revise the DWA methodology by implementing a new method that redefines the objective function as a loss function, allowing the application of gradient descent for optimization. We verified the correctness of our optimization by controlling a robot in an unknown environment with obstacles to visit given positions. Our approach was tested in simulation on ROS and on a real Turtlebot robot, demonstrating improved runtime execution and a less abrupt, more comfortable driving experience.&lt;/p&gt;</description></item><item><title>Motion Control for Aerial and Ground Vehicle Autonomous Platooning</title><link>https://pouceheure.github.io/blog/articles/article_motion-control/</link><pubDate>Sat, 01 Jan 2022 00:00:00 +0000</pubDate><guid>https://pouceheure.github.io/blog/articles/article_motion-control/</guid><description>&lt;h2 id="abstract"&gt;Abstract&lt;/h2&gt;
&lt;p&gt;In this paper, a navigation control for a platoon composed of a ground and an aerial vehicle is proposed. The aim of this work is to define a platooning system that exploits the advantages of both types of robots: while the aerial drone is faster and is not affected by the terrain, the ground vehicle has more autonomy and can carry heavier payloads. The proposed control system allows the aerial vehicle to follow the ground vehicle, maintaining a desired relative position, and to assist in navigation by providing information about the environment. The effectiveness of the proposed approach is demonstrated through simulation and experimental results.&lt;/p&gt;</description></item><item><title>Optimization of the Dynamic Window Approach (DWA)</title><link>https://pouceheure.github.io/blog/projects/project_dwa-optimisation/</link><pubDate>Fri, 10 Dec 2021 00:00:00 +0000</pubDate><guid>https://pouceheure.github.io/blog/projects/project_dwa-optimisation/</guid><description>&lt;h2 id="overview"&gt;Overview&lt;/h2&gt;
&lt;p&gt;The Dynamic Window Approach (DWA) is a reactive motion planning method used in mobile robotics. At each control cycle, it samples admissible velocity pairs $(v, \omega)$ and evaluates them using an objective function, selecting the pair with the highest score.&lt;/p&gt;
&lt;p&gt;The original method presents two main issues:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Discrete sampling&lt;/strong&gt;, which limits precision&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;High computational cost&lt;/strong&gt; in evaluation loops&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;These limitations can be addressed by defining a &lt;strong&gt;convex, differentiable objective function&lt;/strong&gt; and applying &lt;strong&gt;gradient descent&lt;/strong&gt; for continuous optimization.&lt;/p&gt;</description></item><item><title>Drone Control with ROS</title><link>https://pouceheure.github.io/blog/projects/project_drone-control/</link><pubDate>Fri, 01 Feb 2019 00:00:00 +0000</pubDate><guid>https://pouceheure.github.io/blog/projects/project_drone-control/</guid><description>&lt;h2 id="overview"&gt;Overview&lt;/h2&gt;
&lt;p&gt;This project was part of a final-year master&amp;rsquo;s program and involved developing a drone capable of autonomously flying over a predefined area.&lt;br&gt;
The work was split into two main parts: &lt;strong&gt;path planning&lt;/strong&gt; and &lt;strong&gt;control&lt;/strong&gt;.&lt;br&gt;
My responsibility was to control a Parrot Bebop drone using &lt;strong&gt;ROS&lt;/strong&gt;, leveraging an existing ROS package that communicates with Parrot&amp;rsquo;s SDK.&lt;/p&gt;






&lt;figure id="demo-drone"&gt;
 &lt;div class="ratio ratio-16x9" style="max-width: 800px; margin: auto;"&gt;
 &lt;iframe src="https://www.youtube.com/embed/8RcVpDUoFJc" allowfullscreen&gt;&lt;/iframe&gt;
 &lt;/div&gt;
 &lt;figcaption&gt;Video 1 - Drone control demonstration.&lt;/figcaption&gt;
&lt;/figure&gt;




 




 
 

&lt;h2 id="additional-features"&gt;Additional Features&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Controller Watchdog&lt;/strong&gt;: Added a manual override system allowing the operator to take full control of the drone if the autonomous system fails.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Package Improvements&lt;/strong&gt;: Updated the original ROS package to support extra SDK functions from the manufacturer (&lt;a href="https://github.com/AutonomyLab/bebop_autonomy/pull/189"&gt;GitHub Pull Request&lt;/a&gt;), enabling advanced control through the droneâ€™s built-in features.&lt;/li&gt;
&lt;/ul&gt;</description></item></channel></rss>