<!doctype html><html lang=en><head><meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><title>Hugo POUSSEUR</title><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/bootstrap-icons/font/bootstrap-icons.css><link rel=stylesheet href=https://pouceheure.github.io/blog/css/style.css><link rel=stylesheet href=https://pouceheure.github.io/blog/css/color.css><link rel=stylesheet href=https://pouceheure.github.io/blog/css/markdown.css><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.css><link rel=icon type=image/png sizes=32x32 href=https://pouceheure.github.io/blog/favicon//favicon-32.png><link rel=icon type=image/png sizes=16x16 href=https://pouceheure.github.io/blog/favicon//favicon-16.png><link rel=icon type=image/png sizes=64x64 href=https://pouceheure.github.io/blog/favicon//favicon-64.png><link rel=apple-touch-icon sizes=180x180 href=https://pouceheure.github.io/blog/favicon//apple-touch-icon.png><link rel="shortcut icon" href=https://pouceheure.github.io/blog/favicon//favicon.ico><style>@media(max-width:767.98px){main{margin-top:112px}}@media(min-width:768px){main{margin-top:56px}}</style></head><body class=bg-light><header><nav class="navbar navbar-expand-md navbar-dark navbar-custom fixed-top bg-primary text-white p-2"><div class="container-fluid px-4 d-flex align-items-center justify-content-between"><a class="navbar-brand fw-bold" href=https://pouceheure.github.io/blog/>Hugo POUSSEUR</a><div class="d-flex align-items-center social-icons ms-md-0 ms-auto"><a href=https://www.linkedin.com/in/hugo-pousseur/ target=_blank class="text-white me-2"><i class="bi bi-linkedin icon-size"></i>
</a><a href=https://github.com/pouceheure/ target=_blank class="text-white me-2"><i class="bi bi-github icon-size"></i>
</a><button type=button class="btn btn-link text-white me-2 p-0" data-bs-toggle=modal data-bs-target=#contactModal>
<i class="bi bi-chat-fill icon-size"></i>
</button>
<a href="https://www.youtube.com/playlist?list=PLGzYzDkg-SZrXPj-0gGwnueRWiMS9GiOM" target=_blank class=text-white><i class="bi bi-youtube icon-size-lg"></i></a></div><div class="collapse navbar-collapse justify-content-end d-none d-md-flex" id=navbarNav><ul class=navbar-nav><li class=nav-item><a class="nav-link text-white fw-bold me-3" href=https://pouceheure.github.io/blog/>Home</a></li><li class=nav-item><a class="nav-link text-white fw-bold me-3" href=https://pouceheure.github.io/blog/projects/>Projects</a></li><li class=nav-item><a class="nav-link text-white fw-bold me-3" href=https://pouceheure.github.io/blog/articles/><span class="d-none d-lg-inline">Scientific </span>Articles</a></li><li class=nav-item><a class="nav-link text-white fw-bold me-3" href=https://pouceheure.github.io/blog/cv/>About</a></li></ul></div></div></nav><nav class="d-flex d-md-none bg-primary text-white p-2 navbar-dark navbar-custom fixed-top" style=top:56px;z-index:10><div class="container-fluid d-flex justify-content-around"><ul class="navbar-nav flex-row w-100 justify-content-around mb-0"><li class=nav-item><a class="nav-link text-white fw-bold" href=https://pouceheure.github.io/blog/>Home</a></li><li class=nav-item><a class="nav-link text-white fw-bold" href=https://pouceheure.github.io/blog/projects/>Projects</a></li><li class=nav-item><a class="nav-link text-white fw-bold" href=https://pouceheure.github.io/blog/articles/>Articles</a></li><li class=nav-item><a class="nav-link text-white fw-bold" href=https://pouceheure.github.io/blog/cv/>About</a></li></ul></div></nav></header><main class=container><meta name=description content="Modular LiDAR perception pipeline for autonomous driving, built with ROS2 and configurable geometric clustering."><meta name=keywords content="sensing_perception,context_decision,autonomous_vehicle,ros2"><meta name=author content="Hugo POUSSEUR"><meta property="og:title" content="LiDAR Detection & Decision Making"><meta property="og:description" content="Modular LiDAR perception pipeline for autonomous driving, built with ROS2 and configurable geometric clustering."><meta property="og:type" content="article"><meta property="og:url" content="https://pouceheure.github.io/blog/projects/project_autosys_lidar_and_decision/"><meta property="og:image" content="https://pouceheure.github.io/blog/images/decision/thumbnail.png"><link rel=stylesheet href=https://pouceheure.github.io/blog/css/toc.css><div class="container d-flex flex-column flex-md-row align-items-start min-vh-100 gap-4"><aside class="toc w-25 d-none d-lg-block toc-sticky"><div class="toc-container p-3 shadow rounded card d-flex flex-column h-100 card-transparent"><h5 class=toc-title><i class="bi bi-journal-richtext me-1"></i> Contents</h5><nav id=TableOfContents><ul><li><a href=#demonstration>Demonstration</a></li><li><a href=#introduction>Introduction</a></li><li><a href=#critical-zone>Critical Zone</a></li><li><a href=#detection>Detection</a><ul><li><a href=#pipeline>Pipeline</a><ul><li><a href=#main>Main</a></li><li><a href=#main--on-path>Main + On Path</a></li><li><a href=#main---on-critical-zone>Main + On Critical Zone</a></li></ul></li></ul></li><li><a href=#cluster-to-perception>Cluster To Perception</a></li><li><a href=#example-on-real-data>Example on Real Data</a></li></ul></nav></div></aside><article class="markdown-content flex-grow-1 w-100 w-md-75 card-transparent p-0 p-md-3"><h1 class=text-primary>LiDAR Detection & Decision Making</h1><div class="d-flex flex-wrap gap-2"><a href=https://pouceheure.github.io/blog/tags/sensing_perception/ class="btn btn-sm btn-secondary"><i class="bi bi-tag"></i>
<strong style=font-size:1.1em>Sensing & Perception</strong>
</a><a href=https://pouceheure.github.io/blog/tags/context_decision/ class="btn btn-sm btn-secondary"><i class="bi bi-tag"></i>
<strong style=font-size:1.1em>Context Awareness & Decision Making</strong>
</a><a href=https://pouceheure.github.io/blog/tags/autonomous_vehicle/ class="btn btn-sm btn-secondary"><i class="bi bi-tag"></i>
<strong style=font-size:1.1em>Autonomous Vehicle</strong>
</a><a href=https://pouceheure.github.io/blog/tags/ros2/ class="btn btn-sm btn-secondary"><i class="bi bi-tag"></i>
<strong style=font-size:1.1em>ROS2</strong>
</a><span class="btn btn-sm btn-info text-white" style=pointer-events:none><i class="bi bi-code-slash"></i>
<strong style=font-size:1.1em>python</strong></span></div><div class=content><h2 id=demonstration>Demonstration</h2><figure id=video_demo_lidar><div class="ratio ratio-16x9" style=max-width:800px;margin:auto><iframe src=https://www.youtube.com/embed/YJY-9dpiC7o allowfullscreen></iframe></div><figcaption>Video 1 - Video demo of the LiDAR Detection pipeline</figcaption></figure><blockquote><p>Info: A video demonstration of the decision-making system is coming soon.</p></blockquote><h2 id=introduction>Introduction</h2><p>In autonomous driving, the vehicle must be able to make a series of decisions, such as slowing down, stopping at an intersection, or waiting until the road is clear before proceeding.</p><p>These decisions depend on the behavior of other road users. The system must therefore identify surrounding obstacles and determine which ones are relevant for decision-making.</p><p>We define two complementary approaches:</p><ul><li><strong>Perception</strong>: performed using a LiDAR sensor, providing a precise 3D view of the environment.</li><li><strong>Critical zone</strong>: a map-based region that defines where the vehicle should pay special attention to obstacles, depending on the situation.</li></ul><blockquote><p>Disclaimer: This stack was initially developed in Python as a proof of concept (PoC). Once validated, it will be reimplemented in C++ to improve performance and execution speed.</p></blockquote><h2 id=critical-zone>Critical Zone</h2><p>In situations such as approaching a stop sign, yielding, or crossing an uncontrolled intersection, the vehicle must decide whether to proceed or wait.<br>This decision depends on the presence of other vehicles or obstacles within a specific area of interest called the <strong>critical zone</strong>.</p><p>The process is divided into two steps:</p><ol><li>Using the vehicle’s map + Road Rules, identify the lanes necessary to a decision, this defines the <em>critical zone</em>.</li><li>Focus perception and filtering on this zone to determine if it is empty or occupied.</li></ol><p>The decision logic is binary:</p><ul><li>if the critical zone is <strong>empty</strong>, the vehicle can proceed.</li><li>if it is <strong>occupied</strong>, the vehicle must wait or slow down.</li></ul><div class="row justify-content-center"><div class="col-md-6 col-sm-6 col-12 text-center"><figure id=fig-1.1><img src=https://pouceheure.github.io/blog/images/decision/stop_left.png alt="Stop Left" class=img-fluid><figcaption class=text-muted>Fig. 1.1 - Stop Left</figcaption></figure></div><div class="col-md-6 col-sm-6 col-12 text-center"><figure id=fig-1.2><img src=https://pouceheure.github.io/blog/images/decision/stop_right.png alt=" Stop Right." class=img-fluid><figcaption class=text-muted>Fig. 1.2 - Stop Right.</figcaption></figure></div></div><div class="row justify-content-center"><div class="col-md-6 col-sm-6 col-12 text-center"><figure id=fig-2.1><img src=https://pouceheure.github.io/blog/images/decision/roundabout.png alt=Roundabout class=img-fluid><figcaption class=text-muted>Fig. 2.1 - Roundabout</figcaption></figure></div><div class="col-md-6 col-sm-6 col-12 text-center"><figure id=fig-2.2><img src=https://pouceheure.github.io/blog/images/decision/cross.png alt=" Cross." class=img-fluid><figcaption class=text-muted>Fig. 2.2 - Cross.</figcaption></figure></div></div><p>The
<a href=#fig-1>Fig. 1</a>
and
<a href=#fig-2>Fig. 2</a>
illustrate how the critical zone adapts depending on the path geometry and the road rules.
In practice, the critical zone must adapt to the right-of-way rules and the type of road element (e.g., stop sign, intersection, roundabout). This prevents defining an overly strict critical zone that would unnecessarily block the vehicle’s progress.</p><blockquote><p>Legend:</p><ul><li><strong>Red lines</strong>: represent critical zone lines from the map;</li><li><strong>Blue polygones</strong>: represent polygones from the critical zone lines, used for filtering;</li></ul></blockquote><h2 id=detection>Detection</h2><p>The perception stack relies on a <strong>geometric LiDAR-based approach</strong>.<br>Unlike deep-learning methods, this pipeline is deterministic and explainable, every detection can be interpreted and justified.</p><p>Starting from a raw LiDAR point cloud (<code>PCL2</code>), the system identifies a set of obstacles (<code>Perceptions[]</code>) represented by ground-projected polygons.<br>These obstacles can influence the vehicle’s motion when:</p><ul><li>An object lies on the planned driving path;</li><li>An object is within a defined critical zone.</li></ul><blockquote><p>Note: The adaptive control is performed by the perception + the signal obstacle created by the motion manger</p></blockquote><div class=refer-box><i class="bi bi-book"></i> For more details, refer to the <strong>Motion Profile Generation</strong> section of <a href=https://pouceheure.github.io/blog/projects/project_autosys_local-planning/#motion-profile-generation>Planning Project</a>.</div><h3 id=pipeline>Pipeline</h3><p>To filter obstacles along the path or within critical zones, the pipeline generates <code>Perceptions[]</code> objects aligned with the map frame.<br>From these, filtered subsets are computed depending on the driving context.</p><h4 id=main>Main</h4><table><thead><tr><th>Step</th><th>I/O : type (frameid)</th><th>Description</th></tr></thead><tbody><tr><td><strong>1. Point Filtering</strong></td><td><code>PCL2 (lidar)</code> → <code>PCL2 (lidar)</code></td><td>Filters raw LiDAR points by height (<code>z_min</code>, <code>z_max</code>) and range limits to remove ground or distant noise.</td></tr><tr><td><strong>2. Voxel Downsampling</strong></td><td><code>PCL2 (lidar)</code> → <code>PCL2 (lidar)</code></td><td>Reduces cloud density using voxel grid filtering for faster processing.</td></tr><tr><td><strong>3. Clustering (Voxel-based)</strong></td><td><code>PCL2 (lidar)</code> → <code>PCL2 (lidar)</code></td><td>Groups nearby points using voxel-based connected components in 2D/3D space, efficient and stable segmentation.</td></tr><tr><td><strong>4. Cluster Filtering</strong></td><td><code>PCL2 (lidar)</code> → <code>PCL2 (lidar)</code></td><td>Removes clusters below size or height thresholds to discard irrelevant objects.</td></tr><tr><td><strong>5. TF Transform (Clusters)</strong></td><td><code>PCL2 (lidar)</code> → <code>PCL2 (map)</code></td><td>Transforms clustered points to the <code>map</code> frame for spatial consistency.</td></tr><tr><td><strong>6. Road Polygon Filtering</strong></td><td><code>PCL2 (map)</code> → <code>PCL2 (map)</code></td><td>Keeps only clusters overlapping with the road polygons from the map.</td></tr><tr><td><strong>7. Cluster → Perceptions</strong></td><td><code>PCL2 (map)</code> → <code>Perceptions[] (map)</code></td><td>Converts clusters into perception objects with bounding boxes and 2D footprints.</td></tr><tr><td><strong>8. Perception Filtering</strong></td><td><code>Perceptions[] (map)</code> → <code>Perceptions[] (map)</code></td><td>Filters perceptions by area or geometry constraints to retain valid obstacles.</td></tr></tbody></table><h4 id=main--on-path>Main + On Path</h4><table><thead><tr><th>Step</th><th>I/O type (frameid)</th><th>Description</th></tr></thead><tbody><tr><td><strong>9. Path → Polygons (Local)</strong></td><td><code>Path (map)</code> → <code>Polygons[] (map)</code></td><td>Generates polygons around the navigation path for footprint validation and filtering.</td></tr><tr><td><strong>10. Perceptions on Path Filter</strong></td><td><code>Perceptions[] (map)</code> + <code>Polygons[] (map)</code> → <code>Perceptions[] (map)</code></td><td>Keeps only perceptions intersecting the drivable corridor, focusing on relevant obstacles.</td></tr></tbody></table><h4 id=main---on-critical-zone>Main + On Critical Zone</h4><table><thead><tr><th>Step</th><th>I/O type (frameid)</th><th>Description</th></tr></thead><tbody><tr><td><strong>9. Critical Zone Polygons</strong></td><td><code>Map (map)</code> → <code>Polygons[] (map)</code></td><td>Converts the map-defined critical zone into polygons used for decision-making.</td></tr><tr><td><strong>10. Perceptions in Critical Zone Filter</strong></td><td><code>Perceptions[] (map)</code> + <code>Polygons[] (map)</code> → <code>Perceptions[] (map)</code></td><td>Keeps only perceptions located inside critical zones for safety evaluation.</td></tr></tbody></table><h2 id=cluster-to-perception>Cluster To Perception</h2><p>The step <strong>7. Cluster → Perceptions</strong> represents the transformation from clustered PointCloud2 data, meaning the point cloud includes a cluster ID channel, into an array of Perception objects.</p><p>Each Perception defines the shape and footprint of an obstacle.
The footprint is represented by a polygon.</p><p>Footprint Polygons Steps:</p><ol><li><strong>Make a grid</strong>: Create a blank image that covers all the points.</li><li><strong>Plot the points</strong>: Mark each point as a white pixel on the grid.</li><li><strong>Morphological closing</strong>: Use dilation + erosion to connect nearby points and fill small gaps.</li><li><strong>Smooth</strong>: Optionally blur the image to make shapes more rounded.</li><li><strong>Threshold + dilate</strong>: Convert back to binary and slightly thicken shapes.</li><li><strong>Find contours</strong>: Detect continuous white regions (these become polygon boundaries).</li><li><strong>Convert back to real coordinates</strong>: Map pixel positions to the original coordinate system.</li><li><strong>Clean polygons</strong>: Use Shapely to fix invalid shapes, remove very small ones, and handle multiple regions.</li><li><strong>Return polygons</strong>: Output valid Shapely polygons representing the connected areas of points.</li></ol><h2 id=example-on-real-data>Example on Real Data</h2><figure id=fig-3><img src=https://pouceheure.github.io/blog/images/decision/lidar_detection.png alt width=800><figcaption>Fig. 3 - Perception LiDAR on real data, case: Roundabout</figcaption></figure><p>The
<a href=#fig-3>Fig. 3</a>
illustrates a real-data example.<br>On the left is the camera image showing the driving situation. Note that the camera is <strong>not used</strong> for detection.<br>On the right, the RViz visualization displays the LiDAR-based perception results.</p><blockquote><p>Legend:</p><ul><li><strong>Green lane:</strong> Polygon of the planned path the vehicle will follow.</li><li><strong>Orange lane:</strong> Polygon representing the critical zone.</li><li><strong>Red dots + blue polygon (right):</strong> A cluster detected by the pipeline, close to the map area but ignored after path and critical-zone filtering.</li><li><strong>Green dots + orange shape + blue polygon:</strong> The detected vehicle obstacle located within >the critical zone. The orange overlay indicates that this object lies inside the critical zone.</li></ul></blockquote><figure id=fig-4><img src=https://pouceheure.github.io/blog/images/decision/lidar_front_gi.png alt width=800><figcaption>Fig. 4 - Perception LiDAR on real data, case: Front Building.</figcaption></figure><figure id=fig-5><img src=https://pouceheure.github.io/blog/images/decision/lidar_parking_gi.png alt width=800><figcaption>Fig. 5 - Perception LiDAR on real data, case: Parking.</figcaption></figure><p>The
<a href=#fig-4>Fig. 4</a>
,
<a href=#fig-5>Fig. 5</a>
show different detections examples in different situations.</p><figure id=fig-6><img src=https://pouceheure.github.io/blog/images/decision/lidar_cross_guard.png alt width=800><figcaption>Fig. 6 - Perception LiDAR on real data, case: Through The Barrier</figcaption></figure><p>The
<a href=#fig-6>Fig. 6</a>
shows an example where the pipeline detection can detect a vehicle through the barrier between the ego vehicle and the vehicle detected. The red circle shows the vehicle in the image frame and RViZ display.</p></div></article></div><script src=https://pouceheure.github.io/blog/js/highlight-toc.js></script><script src=https://pouceheure.github.io/blog/js/select-toc-part.js></script><script src=https://pouceheure.github.io/blog/js/select-video.js></script></main><div class="modal fade" id=contactModal tabindex=-1 aria-labelledby=contactModalLabel aria-hidden=true><div class="modal-dialog modal-dialog-centered"><div class=modal-content><div class=modal-header><h5 class=modal-title id=contactModalLabel>Contact me</h5><button type=button class=btn-close data-bs-dismiss=modal aria-label=Close></button></div><div class=modal-body><p class=mb-2>Send me email with subject starting by [contact-web]. Email address: click on reveal (avoid bot scrapping).</p><code id=email data-noise=~ data-email="=02~bj5~Cbp~FWb~nBk~c1V~2cz~V3b~w5y~bnV~Ha">=02~bj5~Cbp~FWb~nBk~c1V~2cz~V3b~w5y~bnV~Ha</code><div class=visually-hidden aria-live=polite id=emailLive></div></div><div class=modal-footer><button id=revealEmailBtn class="btn btn-primary">
Reveal Email
</button>
<button type=button class="btn btn-secondary" data-bs-dismiss=modal>
Close</button></div></div></div></div><script src=https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.js></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/contrib/auto-render.min.js onload=renderMathInElement(document.body)></script><script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script><script src=https://pouceheure.github.io/blog/js/setup-latex.js></script><script src=https://pouceheure.github.io/blog/js/highlight-navbar.js></script><script>highlightNavbar("https://pouceheure.github.io/blog/")</script><script src=https://pouceheure.github.io/blog/js/webmail-reveal.js></script></body></html>