<!doctype html><html lang=en><head><meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><title>Hugo POUSSEUR</title><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/bootstrap-icons/font/bootstrap-icons.css><link rel=stylesheet href=/css/style.css><link rel=stylesheet href=/css/color.css><link rel=stylesheet href=/css/markdown.css><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.css><link rel=icon type=image/png sizes=32x32 href=/favicon//favicon-32.png><link rel=icon type=image/png sizes=16x16 href=/favicon//favicon-16.png><link rel=icon type=image/png sizes=64x64 href=/favicon//favicon-64.png><link rel=apple-touch-icon sizes=180x180 href=/favicon//apple-touch-icon.png><link rel="shortcut icon" href=/favicon//favicon.ico><style>@media(max-width:767.98px){main{margin-top:112px}}@media(min-width:768px){main{margin-top:56px}}</style></head><body class=bg-light><header><nav class="navbar navbar-expand-md navbar-dark navbar-custom fixed-top bg-primary text-white p-2"><div class="container-fluid px-4 d-flex align-items-center justify-content-between"><a class="navbar-brand fw-bold" href=/>Hugo POUSSEUR</a><div class="d-flex align-items-center social-icons ms-md-0 ms-auto"><a href=https://www.linkedin.com/in/hugo-pousseur/ target=_blank class="text-white me-2"><i class="bi bi-linkedin icon-size"></i>
</a><a href=https://github.com/pouceheure/ target=_blank class="text-white me-2"><i class="bi bi-github icon-size"></i>
</a><a href=# class="text-white me-2" data-bs-toggle=modal data-bs-target=#contactModal><i class="bi bi-chat-fill icon-size"></i>
</a><a href="https://www.youtube.com/playlist?list=PLGzYzDkg-SZrXPj-0gGwnueRWiMS9GiOM" target=_blank class=text-white><i class="bi bi-youtube icon-size-lg"></i></a></div><div class="collapse navbar-collapse justify-content-end d-none d-md-flex" id=navbarNav><ul class=navbar-nav><li class=nav-item><a class="nav-link text-white fw-bold me-3" href=/>Home</a></li><li class=nav-item><a class="nav-link text-white fw-bold me-3" href=/projects/>Projects</a></li><li class=nav-item><a class="nav-link text-white fw-bold me-3" href=/articles/><span class="d-none d-lg-inline">Scientific </span>Articles</a></li><li class=nav-item><a class="nav-link text-white fw-bold me-3" href=/cv/>About</a></li></ul></div></div></nav><nav class="d-flex d-md-none bg-primary text-white p-2 navbar-dark navbar-custom fixed-top" style=top:56px;z-index:10><div class="container-fluid d-flex justify-content-around"><ul class="navbar-nav flex-row w-100 justify-content-around mb-0"><li class=nav-item><a class="nav-link text-white fw-bold" href=/>Home</a></li><li class=nav-item><a class="nav-link text-white fw-bold" href=/projects/>Projects</a></li><li class=nav-item><a class="nav-link text-white fw-bold" href=/articles/>Articles</a></li><li class=nav-item><a class="nav-link text-white fw-bold" href=/cv/>About</a></li><li class=nav-item><button class="nav-link text-white fw-bold btn btn-link p-0" data-bs-toggle=modal data-bs-target=#contactModal>
<i class="bi bi-chat-fill"></i></button></li></ul></div></nav></header><main class=container><meta name=description content="Leg detection using an LSTM classifier on spatial LiDAR sequences."><meta name=keywords content="ai_ml,core_technologies,ros,sensing_perception"><meta name=author content="Hugo POUSSEUR"><meta property="og:title" content="Leg Detection Using a 2D LiDAR"><meta property="og:description" content="Leg detection using an LSTM classifier on spatial LiDAR sequences."><meta property="og:type" content="article"><meta property="og:url" content="https://pouceheure.github.io/blog/projects/project_human-legs-detection/"><meta property="og:image" content="https://pouceheure.github.io/images/legs-detection/thumbnail.png"><link rel=stylesheet href=/css/toc.css><div class="container d-flex flex-column flex-md-row align-items-start min-vh-100 gap-4"><aside class="toc w-25 d-none d-lg-block toc-sticky"><div class="toc-container p-3 shadow rounded card d-flex flex-column h-100 card-transparent"><h5 class=toc-title><i class="bi bi-journal-richtext me-1"></i> Contents</h5><nav id=TableOfContents><ul><li><a href=#overview>Overview</a><ul><li><a href=#source-code>Source Code</a></li><li><a href=#context>Context</a></li><li><a href=#pipeline>Pipeline</a></li></ul></li><li><a href=#input-representation>Input Representation</a><ul><li><a href=#lstm-model>LSTM Model</a></li><li><a href=#model-output-and-training-loss>Model Output and Training Loss</a></li></ul></li><li><a href=#estimating-the-center-of-a-detected-cluster>Estimating the Center of a Detected Cluster</a></li><li><a href=#training-and-augmentation>Training and Augmentation</a></li><li><a href=#integration>Integration</a></li></ul></nav></div></aside><article class="markdown-content flex-grow-1 w-100 w-md-75"><h1 class=text-primary>Leg Detection Using a 2D LiDAR</h1><div class="d-flex flex-wrap gap-2"><a href=/tags/ai_ml/ class="btn btn-sm btn-secondary"><i class="bi bi-tag"></i>
<strong style=font-size:1.1em>AI & ML</strong>
</a><a href=/tags/core_technologies/ class="btn btn-sm btn-secondary"><i class="bi bi-tag"></i>
<strong style=font-size:1.1em>Core Technologies</strong>
</a><a href=/tags/ros/ class="btn btn-sm btn-secondary"><i class="bi bi-tag"></i>
<strong style=font-size:1.1em>ROS</strong>
</a><a href=/tags/sensing_perception/ class="btn btn-sm btn-secondary"><i class="bi bi-tag"></i>
<strong style=font-size:1.1em>Sensing & Perception</strong>
</a><span class="btn btn-sm btn-info text-white" style=pointer-events:none><i class="bi bi-code-slash"></i>
<strong style=font-size:1.1em>python</strong></span></div><div class="align-items-center mt-2"><a href=https://github.com/PouceHeure/ros_detection_legs rel="noopener noreferrer" class="btn btn-success" style=text-decoration:none><span style=text-decoration:underline><i class="bi bi-github"></i></span>
<span style=display:inline-block;width:.25em></span>
<span style=text-decoration:underline>Source Code</span></a></div><div class=content><h2 id=overview>Overview</h2><h3 id=source-code>Source Code</h3><p>In addition to the main GitHub project, the following submodules are available:</p><ul><li><em>Dataset of scanned legs (with labels):</em> <a href=https://github.com/PouceHeure/dataset_lidar2D_legs>https://github.com/PouceHeure/dataset_lidar2D_legs</a></li><li><em>Labeling GUI tool:</em> <a href=https://github.com/PouceHeure/lidar_tool_label>https://github.com/PouceHeure/lidar_tool_label</a></li><li><em>Radar interface:</em> <a href=https://github.com/PouceHeure/ros_pygame_radar_2D>https://github.com/PouceHeure/ros_pygame_radar_2D</a></li></ul><h3 id=context>Context</h3><p>This project detects human legs from 2D LiDAR scans using a Recurrent Neural Network (RNN) with LSTM cells. The LSTM processes each scan as a spatial sequence ordered by angle $ \theta $, not as a time series. The model learns local shape patterns in polar space that are typical of legs.</p><h3 id=pipeline>Pipeline</h3><p>End-to-end steps:</p><ol><li>Data acquisition with a 2D LiDAR</li><li>Custom labeling and annotation</li><li>Spatial clustering and sequence building</li><li>LSTM training</li><li>Real-time ROS integration and visualization</li></ol><figure id=legs-demo><div class="ratio ratio-16x9" style=max-width:800px;margin:auto><iframe src=https://www.youtube.com/embed/KcfxU6_UrOo allowfullscreen></iframe></div><figcaption>Video 1 - Video demo: legs detection from a 2D LiDAR scan.</figcaption></figure><p>Demo:
<a href=#legs-demo>Video 1: Video demo: legs detection from a 2D LiDAR scan.</a>
.</p><h2 id=input-representation>Input Representation</h2><p>Each LiDAR scan is a set of polar points:</p><blockquote><div class=equation-scroll-wrapper><div class=equation-block>$$
P_i = (\theta_i, r_i)
\quad \text{Eq (1)} $$</div></div></blockquote><p>where $ \theta_i $ is the angle of point $ i $ (radians) and $ r_i $ is its range (meters).</p><p>To form model inputs, points are grouped into clusters using two metrics:</p><ol><li>Polar-plane Euclidean distance</li><li>Radial difference</li></ol><blockquote><div class=equation-scroll-wrapper><div class=equation-block>$$
d_{\text{polar}}(P_i, P_j) = \sqrt{r_i^{2} + r_j^{2} - 2 r_i r_j \cos(\theta_i - \theta_j)}
\quad \text{Eq (2)} $$</div></div></blockquote><blockquote><div class=equation-scroll-wrapper><div class=equation-block>$$
d_{\text{radius}}(P_i, P_j) = |r_i - r_j|
\quad \text{Eq (3)} $$</div></div></blockquote><p>Two points are connected if both metrics are below thresholds <code>limit_distance</code> and <code>limit_radius</code>. Each connected component (cluster) is encoded as an angle-ordered sequence:</p><blockquote><div class=equation-scroll-wrapper><div class=equation-block>$$
C = \{ P_1, P_2, \dots, P_n \}
\quad \text{Eq (4)} $$</div></div></blockquote><figure id=fig-1><img src=https://raw.githubusercontent.com/PouceHeure/ros_detection_legs/master/.doc/graph/segmentation.png alt width=500><figcaption>Fig. 1 - Cluster variables representation.</figcaption></figure><p>The
<a href=#fig-1>Fig. 1</a>
defines the variables used to describe each cluster. Each cluster sequence $ C $ is then passed to the LSTM classifier.</p><h3 id=lstm-model>LSTM Model</h3><p>The LSTM captures dependencies along the ordered scan. Typical leg returns form smooth, narrow arcs or symmetric curves. The network relies on:</p><ul><li>Changes in $ r $ along $ \theta $</li><li>Relative angular spacing of points</li><li>Local curvature cues derived from neighboring points</li></ul><h3 id=model-output-and-training-loss>Model Output and Training Loss</h3><p>The classifier outputs a probability $ \hat{y} \in [0, 1] $ for the class “leg.” A prediction threshold (for example, $0.5$) yields a binary label:</p><ul><li><code>1</code> for leg</li><li><code>0</code> otherwise</li></ul><p>Training uses binary cross-entropy:</p><blockquote><div class=equation-scroll-wrapper><div class=equation-block>$$
\mathcal{L}_{\text{BCE}} = -\big( y \log(\hat{y}) + (1 - y) \log(1 - \hat{y}) \big)
\quad \text{Eq (5)} $$</div></div></blockquote><p>where $ y \in {0, 1} $ is the ground-truth label and $ \hat{y} $ is the predicted probability.</p><h2 id=estimating-the-center-of-a-detected-cluster>Estimating the Center of a Detected Cluster</h2><p>For each cluster $C = \{ P_1, P_2, \dots, P_n \}$ with $ P_i = (\theta_i, r_i) $ that is classified as a leg, the polar center is computed as:</p><blockquote><div class=equation-scroll-wrapper><div class=equation-block>$$
\theta_{\text{center}} = \frac{1}{n} \sum_{i=1}^{n} \theta_i
\quad \text{Eq (6)} $$</div></div></blockquote><blockquote><div class=equation-scroll-wrapper><div class=equation-block>$$
r_{\text{center}} = \frac{1}{n} \sum_{i=1}^{n} r_i
\quad \text{Eq (7)} $$</div></div></blockquote><p>Here, $ \theta_{\text{center}} $ is the mean angular position and $ r_{\text{center}} $ is the mean range of the cluster, with $ n $ the number of points. The polar center can be converted to Cartesian coordinates:</p><blockquote><div class=equation-scroll-wrapper><div class=equation-block>$$
x = r_{\text{center}} \cos(\theta_{\text{center}}), \quad
y = r_{\text{center}} \sin(\theta_{\text{center}})
\quad \text{Eq (8)} $$</div></div></blockquote><p>These estimates are used during evaluation and in real-time ROS inference.</p><h2 id=training-and-augmentation>Training and Augmentation</h2><p>Positive clusters are augmented by rotation to improve generalization:</p><blockquote><div class=equation-scroll-wrapper><div class=equation-block>$$
\theta_i' = \theta_i + \Delta \theta
\quad \text{Eq (9)} $$</div></div></blockquote><figure id=fig-2><img src=https://raw.githubusercontent.com/PouceHeure/ros_detection_legs/master/.doc/graph/raising.png alt width=300><figcaption>Fig. 2 - Data augmentation by rotating clusters.</figcaption></figure><p>The
<a href=#fig-2>Fig. 2</a>
shows how rotation preserves a cluster’s internal structure while changing its orientation.</p><p>To address class imbalance, positives are upsampled. The final dataset size is:</p><blockquote><div class=equation-scroll-wrapper><div class=equation-block>$$
\text{Final dataset size} = N + K \cdot N_{\text{positive}}
\quad \text{Eq (10)} $$</div></div></blockquote><p>where $ N $ is the original dataset size, $ N_{\text{positive}} $ the number of positive clusters, and $ K $ the number of augmentation steps.</p><p>Training curves:</p><figure id=fig-3><img src=https://raw.githubusercontent.com/PouceHeure/ros_detection_legs/master/model/train/evaluation.png alt width=500><figcaption>Fig. 3 - Training curves.</figcaption></figure><p>The
<a href=#fig-3>Fig. 3</a>
summarizes optimization progress and generalization metrics.</p><h2 id=integration>Integration</h2><p>A ROS node, <code>detector_node</code>, subscribes to <code>/scan</code>. Incoming scans are clustered and converted into sequences, which are classified by the trained LSTM. Detected positions are published to <code>/radar</code>.</p><figure id=fig-4><img src=https://raw.githubusercontent.com/PouceHeure/ros_detection_legs/master/.doc/graph/prediction_ros.png alt width=500><figcaption>Fig. 4 - Prediction pipeline inside the ROS node.</figcaption></figure><p>The
<a href=#fig-4>Fig. 4</a>
outlines the flow from scan to detections.</p><p>Clustering runs inside the subscriber callback so that raw LiDAR data are transformed into sequences before inference.</p><figure id=fig-5><img src=https://raw.githubusercontent.com/PouceHeure/ros_detection_legs/master/.doc/graph/prediction.png alt width=500><figcaption>Fig. 5 - Clustering and sequence creation for LSTM input during prediction.</figcaption></figure><p>The
<a href=#fig-5>Fig. 5</a>
details the conversion from scan points to LSTM-ready sequences. For each positive cluster, the center computation above provides the estimated position.</p></div></article></div><script src=/js/highlight-toc.js></script><script src=/js/select-toc-part.js></script><script src=/js/select-video.js></script></main><div class="modal fade" id=contactModal tabindex=-1 aria-labelledby=contactModalLabel aria-hidden=true><div class="modal-dialog modal-dialog-centered"><div class=modal-content><div class=modal-header><h5 class=modal-title id=contactModalLabel>Contact me</h5><button type=button class=btn-close data-bs-dismiss=modal aria-label=Close></button></div><div class=modal-body><p class=mb-2>Send me email with object starting by [contact-web], email address (click on reveal)</p><code id=email data-noise=~ data-email="=02~bj5~Cbp~FWb~nBk~c1V~2cz~V3b~w5y~bnV~Ha">=02~bj5~Cbp~FWb~nBk~c1V~2cz~V3b~w5y~bnV~Ha</code><div class=visually-hidden aria-live=polite id=emailLive></div></div><div class=modal-footer><button id=revealEmailBtn class="btn btn-primary">
Reveal Email
</button>
<button type=button class="btn btn-secondary" data-bs-dismiss=modal>
Close</button></div></div></div></div><script src=https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.js></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/contrib/auto-render.min.js onload=renderMathInElement(document.body)></script><script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script><script src=/js/setup-latex.js></script><script src=/js/highlight-navbar.js></script><script src=/js/webmail-reveal.js></script></body></html>