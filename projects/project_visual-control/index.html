<!doctype html><html lang=en><head><meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><title>Hugo POUSSEUR</title><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/bootstrap-icons/font/bootstrap-icons.css><link rel=stylesheet href=https://pouceheure.github.io/blog/css/style.css><link rel=stylesheet href=https://pouceheure.github.io/blog/css/color.css><link rel=stylesheet href=https://pouceheure.github.io/blog/css/markdown.css><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.css><link rel=icon type=image/png sizes=32x32 href=https://pouceheure.github.io/blog/favicon//favicon-32.png><link rel=icon type=image/png sizes=16x16 href=https://pouceheure.github.io/blog/favicon//favicon-16.png><link rel=icon type=image/png sizes=64x64 href=https://pouceheure.github.io/blog/favicon//favicon-64.png><link rel=apple-touch-icon sizes=180x180 href=https://pouceheure.github.io/blog/favicon//apple-touch-icon.png><link rel="shortcut icon" href=https://pouceheure.github.io/blog/favicon//favicon.ico><style>@media(max-width:767.98px){main{margin-top:112px}}@media(min-width:768px){main{margin-top:56px}}</style></head><body class=bg-light><header><nav class="navbar navbar-expand-md navbar-dark navbar-custom fixed-top bg-primary text-white p-2"><div class="container-fluid px-4 d-flex align-items-center justify-content-between"><a class="navbar-brand fw-bold" href=https://pouceheure.github.io/blog/>Hugo POUSSEUR</a><div class="d-flex align-items-center social-icons ms-md-0 ms-auto"><a href=https://www.linkedin.com/in/hugo-pousseur/ target=_blank class="text-white me-2"><i class="bi bi-linkedin icon-size"></i>
</a><a href=https://github.com/pouceheure/ target=_blank class="text-white me-2"><i class="bi bi-github icon-size"></i>
</a><button type=button class="btn btn-link text-white me-2 p-0" data-bs-toggle=modal data-bs-target=#contactModal>
<i class="bi bi-chat-fill icon-size"></i>
</button>
<a href="https://www.youtube.com/playlist?list=PLGzYzDkg-SZrXPj-0gGwnueRWiMS9GiOM" target=_blank class=text-white><i class="bi bi-youtube icon-size-lg"></i></a></div><div class="collapse navbar-collapse justify-content-end d-none d-md-flex" id=navbarNav><ul class=navbar-nav><li class=nav-item><a class="nav-link text-white fw-bold me-3" href=https://pouceheure.github.io/blog/>Home</a></li><li class=nav-item><a class="nav-link text-white fw-bold me-3" href=https://pouceheure.github.io/blog/projects/>Projects</a></li><li class=nav-item><a class="nav-link text-white fw-bold me-3" href=https://pouceheure.github.io/blog/articles/><span class="d-none d-lg-inline">Scientific </span>Articles</a></li><li class=nav-item><a class="nav-link text-white fw-bold me-3" href=https://pouceheure.github.io/blog/cv/>About</a></li></ul></div></div></nav><nav class="d-flex d-md-none bg-primary text-white p-2 navbar-dark navbar-custom fixed-top" style=top:56px;z-index:10><div class="container-fluid d-flex justify-content-around"><ul class="navbar-nav flex-row w-100 justify-content-around mb-0"><li class=nav-item><a class="nav-link text-white fw-bold" href=https://pouceheure.github.io/blog/>Home</a></li><li class=nav-item><a class="nav-link text-white fw-bold" href=https://pouceheure.github.io/blog/projects/>Projects</a></li><li class=nav-item><a class="nav-link text-white fw-bold" href=https://pouceheure.github.io/blog/articles/>Articles</a></li><li class=nav-item><a class="nav-link text-white fw-bold" href=https://pouceheure.github.io/blog/cv/>About</a></li></ul></div></nav></header><main class=container><meta name=description content="Control a vehicle using only camera images by linking lane features to vehicle speed and orientation."><meta name=keywords content="autonomous_vehicle,ai_ml,control_optimization,robotics_autonomy,sensing_perception"><meta name=author content="Hugo POUSSEUR"><meta property="og:title" content="Visual Control Applied To Autonomous Vehicle"><meta property="og:description" content="Control a vehicle using only camera images by linking lane features to vehicle speed and orientation."><meta property="og:type" content="article"><meta property="og:url" content="https://pouceheure.github.io/blog/projects/project_visual-control/"><meta property="og:image" content="https://pouceheure.github.io/blog/images/visual-control/situation_img.png"><link rel=stylesheet href=https://pouceheure.github.io/blog/css/toc.css><div class="container d-flex flex-column flex-md-row align-items-start min-vh-100 gap-4"><aside class="toc w-25 d-none d-lg-block toc-sticky"><div class="toc-container p-3 shadow rounded card d-flex flex-column h-100 card-transparent"><h5 class=toc-title><i class="bi bi-journal-richtext me-1"></i> Contents</h5><nav id=TableOfContents><ul><li><a href=#abstract>Abstract</a></li><li><a href=#visual-servoing-concepts>Visual Servoing Concepts</a></li><li><a href=#feature-extraction-from-camera-image>Feature Extraction from Camera Image</a><ul><li><a href=#selection-of-target-point>Selection of Target Point</a></li><li><a href=#computation-of-x-and-y>Computation of X and Y</a></li><li><a href=#estimation-of-theta>Estimation of Theta</a></li><li><a href=#final-feature-vector>Final Feature Vector</a></li></ul></li><li><a href=#lane-detection-via-deep-learning>Lane Detection via Deep Learning</a></li><li><a href=#from-feature-to-control>From Feature to Control</a><ul><li><a href=#velocity-computation>Velocity Computation</a></li><li><a href=#control-law>Control Law</a></li></ul></li><li><a href=#control-in-real-time>Control in Real-Time</a></li><li><a href=#integration-into-the-dynamic-window-approach-dwa>Integration into the Dynamic Window Approach (DWA)</a><ul><li><a href=#goal-function>Goal Function</a></li><li><a href=#obstacle-function>Obstacle Function</a></li><li><a href=#results>Results</a></li></ul></li></ul></nav></div></aside><article class="markdown-content flex-grow-1 w-100 w-md-75"><h1 class=text-primary>Visual Control Applied To Autonomous Vehicle</h1><div class="d-flex flex-wrap gap-2"><a href=https://pouceheure.github.io/blog/tags/autonomous_vehicle/ class="btn btn-sm btn-secondary"><i class="bi bi-tag"></i>
<strong style=font-size:1.1em>Autonomous Vehicle</strong>
</a><a href=https://pouceheure.github.io/blog/tags/ai_ml/ class="btn btn-sm btn-secondary"><i class="bi bi-tag"></i>
<strong style=font-size:1.1em>AI & ML</strong>
</a><a href=https://pouceheure.github.io/blog/tags/control_optimization/ class="btn btn-sm btn-secondary"><i class="bi bi-tag"></i>
<strong style=font-size:1.1em>Control & Optimization</strong>
</a><a href=https://pouceheure.github.io/blog/tags/robotics_autonomy/ class="btn btn-sm btn-secondary"><i class="bi bi-tag"></i>
<strong style=font-size:1.1em>Robotics & Autonomy</strong>
</a><a href=https://pouceheure.github.io/blog/tags/sensing_perception/ class="btn btn-sm btn-secondary"><i class="bi bi-tag"></i>
<strong style=font-size:1.1em>Sensing & Perception</strong>
</a><span class="btn btn-sm btn-info text-white" style=pointer-events:none><i class="bi bi-code-slash"></i>
<strong style=font-size:1.1em>python</strong></span></div><div class=content><h2 id=abstract>Abstract</h2><p>As part of a research project, I contributed to the development and testing of a <strong>Visual Servoing</strong> framework, a control strategy that links image feature variations to robot velocities. Applied to autonomous vehicles, this approach enables lane-keeping purely from camera input, without GPS or map dependency. It integrates lane detection using deep learning and transforms the output into features used in a control law that centers the vehicle in the lane.</p><p>The system was validated on simulated and real data, showing robust behavior in diverse road scenarios.</p><figure><div class="ratio ratio-16x9" style=max-width:800px;margin:auto><iframe src=https://www.youtube.com/embed/VPxDvUqFA-o allowfullscreen></iframe></div><figcaption>Video 1 - Video demo, visual control pipeline on real data, from the camera to steering wheel angle.</figcaption></figure><h2 id=visual-servoing-concepts>Visual Servoing Concepts</h2><p>Visual Servoing (VS) refers to the use of visual information, typically from a camera, to control the motion of a robot. The control law is derived from the error between a current image feature $s$ and a desired one $s^*$:</p><blockquote><div class=equation-scroll-wrapper><div class=equation-block>$$
e(t) = s(t) - s^*
\quad \text{Eq (1)} $$</div></div></blockquote><p>The relationship between the change in feature and the robot velocity $u_c$ is modeled by the interaction matrix $L_s$:</p><blockquote><div class=equation-scroll-wrapper><div class=equation-block>$$
\dot{s}(t) = L_s u_c
\quad \text{Eq (2)} $$</div></div></blockquote><p>To ensure convergence, exponential decay is applied to the error:</p><blockquote><div class=equation-scroll-wrapper><div class=equation-block>$$
\dot{e}(t) = -\lambda L_s^+ e(t)
\quad \text{Eq (3)} $$</div></div></blockquote><p>Where $L_s^+$ is the Mooreâ€“Penrose pseudo-inverse of $L_s$.</p><h2 id=feature-extraction-from-camera-image>Feature Extraction from Camera Image</h2><p>The visual servoing controller relies on three geometric features computed from the lane detection in the image:</p><ul><li>$X$: lateral offset between the image center and a selected target point on the lane</li><li>$Y$: vertical position (depth proxy) of the point in the image</li><li>$\Theta$: angular deviation between the lane direction and the vertical image axis</li></ul><p>These features are extracted in the image frame and used as input for control.</p><h3 id=selection-of-target-point>Selection of Target Point</h3><p>The detected lane is assumed to provide a centerline defined as a curve in image space. A single <strong>target point</strong> is selected at a fixed vertical distance from the bottom of the image (typically 3/4 of the image height), denoted $v_t$.</p><p>The horizontal position of this point is obtained by evaluating the lane curve at $v_t$:</p><blockquote><div class=equation-scroll-wrapper><div class=equation-block>$$
u_t = f(v_t)
\quad \text{Eq (4)} $$</div></div></blockquote><p>Where $f$ is the polynomial fitted to the detected centerline.</p><h3 id=computation-of-x-and-y>Computation of X and Y</h3><p>The coordinates $(u_t, v_t)$ are image pixel coordinates. They are normalized relative to the optical center $(c_x, c_y)$ and focal lengths $(f_x, f_y)$:</p><blockquote><div class=equation-scroll-wrapper><div class=equation-block>$$
X = \frac{u_t - c_x}{f_x}, \quad Y = \frac{v_t - c_y}{f_y}
\quad \text{Eq (5)} $$</div></div></blockquote><p>$X$ corresponds to the lateral displacement in the image, and $Y$ is a proxy for depth.</p><h3 id=estimation-of-theta>Estimation of Theta</h3><p>To compute the orientation of the lane at the target point, the tangent to the fitted curve is calculated:</p><blockquote><div class=equation-scroll-wrapper><div class=equation-block>$$
\Theta = \arctan\left( \frac{df}{dv}(v_t) \right)
\quad \text{Eq (6)} $$</div></div></blockquote><p>This angle represents the deviation between the lane direction and the vertical axis of the image.</p><h3 id=final-feature-vector>Final Feature Vector</h3><p>The visual features used in control are then assembled as:</p><blockquote><div class=equation-scroll-wrapper><div class=equation-block>$$
s = [X, Y, \Theta]^T
\quad \text{Eq (7)} $$</div></div></blockquote><p>This vector $s$ is compared to a desired reference $s^*$ to compute the control error.</p><figure id=fig-1><img src=https://pouceheure.github.io/blog/images/visual-control/148.png alt width=600><figcaption>Fig. 1 - Visual Servoing features extracted from real image: centerline and tangent estimation. Target point is sampled along the lane and used to define visual features.</figcaption></figure><h2 id=lane-detection-via-deep-learning>Lane Detection via Deep Learning</h2><div class=refer-box><i class="bi bi-book"></i> For more details, refer to the project <strong><a href=https://pouceheure.github.io/blog/projects/project_lane-detection/>Lane Detection</a></strong></div><p>The first step in feature extraction is robust lane detection. Instead of relying on geometric models, which are brittle in poor lighting or degraded markings, a convolutional autoencoder trained on the CULane dataset is employed.</p><p>The model produces binary segmentation maps for lane positions, which are then used to compute the path to follow.</p><figure id=fig-2><img src=https://pouceheure.github.io/blog/images/lane-detection/models.png alt width=600><figcaption>Fig. 2 - Autoencoder-based lane detection architecture used for generating binary masks of lane boundaries.</figcaption></figure><h2 id=from-feature-to-control>From Feature to Control</h2><h3 id=velocity-computation>Velocity Computation</h3><p>Given the extracted features $s = [X, Y, \Theta]$, the control goal is to minimize the error $e = s - s^*$. The control output $u_r = [v, w]^T$ includes linear and angular velocity commands.</p><p>The time derivative of the features is related to the robot velocity by:</p><blockquote><div class=equation-scroll-wrapper><div class=equation-block>$$
\dot{s} = L_s(X, Y, \Theta) C_{TR} u_r
\quad \text{Eq (8)} $$</div></div></blockquote><p>Where $C_{TR}$ is the transformation matrix between robot and camera frames.</p><h3 id=control-law>Control Law</h3><p>Two control configurations are used depending on the desired feature.</p><p>When the vehicle is far from the lane, the feature point appears above the image bottom, requiring the <strong>column controller</strong> to guide the vehicle back.</p><p>When the feature point remains near the bottom of the image, the <strong>row controller</strong> is used to maintain lane position.</p><p><strong>Row controller</strong> for $s^* = [0, Y, 0]$:</p><blockquote><div class=equation-scroll-wrapper><div class=equation-block>$$
w = -B_{row}^+ \left(
\begin{bmatrix}
K_X e_X \\
K_\Theta e_\Theta
\end{bmatrix} + A_{row} v_d
\right)
\quad \text{Eq (9)} $$</div></div></blockquote><p><strong>Column controller</strong> for $s^* = [X, 0, \pm \frac{\pi}{4}]$:</p><blockquote><div class=equation-scroll-wrapper><div class=equation-block>$$
w = -B_{col}^+ \left(
\begin{bmatrix}
K_Y e_Y \\
K_\Theta e_\Theta
\end{bmatrix} + A_{col} v_d
\right)
\quad \text{Eq (10)} $$</div></div></blockquote><p>Where $v_d$ is the desired forward velocity and the matrices $A$ and $B$ are derived from the interaction matrix and transformation.</p><h2 id=control-in-real-time>Control in Real-Time</h2><p>The computed velocities are sent to the vehicle&rsquo;s motion controller. The system operates in real time and does not depend on external GPS or map input, ensuring portability and robustness in unmapped environments.</p><figure><div class="ratio ratio-16x9" style=max-width:800px;margin:auto><iframe src=https://www.youtube.com/embed/UZjCYTLoMOw allowfullscreen></iframe></div><figcaption>Video 2 - Demonstration of Visual Servoing Lane Following on test track</figcaption></figure><h2 id=integration-into-the-dynamic-window-approach-dwa>Integration into the Dynamic Window Approach (DWA)</h2><div class=refer-box><i class="bi bi-book"></i> For more details, refer to the project <strong><a href=https://pouceheure.github.io/blog/projects/project_dwa-optimisation/>Optimization of The Dynamic Window Approach (DWA)</a></strong></div><p>The Visual Servoing framework can be integrated into a classical Dynamic Window Approach (DWA) by modifying the objective function. Rather than relying solely on geometric terms like heading angle or distance to a local goal, the <strong>Visual Servoing error</strong> is incorporated as part of the scoring function.</p><h3 id=goal-function>Goal Function</h3><p>Each motion command candidate $(v, w)$ generated by the DWA is evaluated not only for obstacle avoidance and kinematic feasibility, but also based on how well it aligns the robot with the desired image-based features.</p><blockquote><div class=equation-scroll-wrapper><div class=equation-block>$$
J_{vs}(v, w) = \| s(v, w) - s^* \|
\quad \text{Eq (11)} $$</div></div></blockquote><p>This VS-based cost is added to the standard DWA cost terms:</p><blockquote><div class=equation-scroll-wrapper><div class=equation-block>$$
J_{\text{total}} = \alpha J_{\text{obs}} + \beta J_{\text{vel}} + \gamma J_{vs}
\quad \text{Eq (12)} $$</div></div></blockquote><p>Where:</p><ul><li>$J_{\text{obs}}$ evaluates obstacle clearance</li><li>$J_{\text{vel}}$ encourages high velocity</li><li>$J_{vs}$ penalizes deviation from visual servoing objectives</li><li>$\alpha$, $\beta$, $\gamma$ are tunable gains</li></ul><h3 id=obstacle-function>Obstacle Function</h3><p>As explained in the project <a href=https://pouceheure.github.io/blog/projects/project_dwa-optimisation/>Optimization Of The Dynamic Window Approach (DWA)</a>, the obstacle avoidance function requires a safety angle to be determined.</p><p>The following figure illustrates the computation of this angle. The angle must satisfy constraints to:</p><ul><li>avoid collisions;</li><li>maintain a safe distance from obstacles;</li><li>remain within navigable space.</li></ul><p>The last constraint is critical, as omitting it could result in the vehicle leaving the road.</p><div class="row justify-content-center"><div class="col-md-6 col-sm-6 col-12 text-center"><figure id=fig-3.1><img src=https://pouceheure.github.io/blog/images/visual-control/function_obstacle.png alt="Obstacle avoidance obstacle explained." class=img-fluid><figcaption class=text-muted>Fig. 3.1 - Obstacle avoidance obstacle explained.</figcaption></figure></div><div class="col-md-6 col-sm-6 col-12 text-center"><figure id=fig-3.2><img src=https://pouceheure.github.io/blog/images/visual-control/limitation_space.png alt="Limitation space from perception." class=img-fluid><figcaption class=text-muted>Fig. 3.2 - Limitation space from perception.</figcaption></figure></div></div><h3 id=results>Results</h3><p>By integrating $J_{vs}$, the DWA selects trajectories that are not only dynamically valid and safe, but also visually consistent with the lane-following objective defined by the Visual Servoing framework.</p><figure><div class="ratio ratio-16x9" style=max-width:800px;margin:auto><iframe src=https://www.youtube.com/embed/TMXqmAW_N_o allowfullscreen></iframe></div><figcaption>Video 3 - Demonstration of DWA + Visual Servoing in simulation.</figcaption></figure></div></article></div><script src=https://pouceheure.github.io/blog/js/highlight-toc.js></script><script src=https://pouceheure.github.io/blog/js/select-toc-part.js></script><script src=https://pouceheure.github.io/blog/js/select-video.js></script></main><div class="modal fade" id=contactModal tabindex=-1 aria-labelledby=contactModalLabel aria-hidden=true><div class="modal-dialog modal-dialog-centered"><div class=modal-content><div class=modal-header><h5 class=modal-title id=contactModalLabel>Contact me</h5><button type=button class=btn-close data-bs-dismiss=modal aria-label=Close></button></div><div class=modal-body><p class=mb-2>Send me email with object starting by [contact-web], email address (click on reveal)</p><code id=email data-noise=~ data-email="=02~bj5~Cbp~FWb~nBk~c1V~2cz~V3b~w5y~bnV~Ha">=02~bj5~Cbp~FWb~nBk~c1V~2cz~V3b~w5y~bnV~Ha</code><div class=visually-hidden aria-live=polite id=emailLive></div></div><div class=modal-footer><button id=revealEmailBtn class="btn btn-primary">
Reveal Email
</button>
<button type=button class="btn btn-secondary" data-bs-dismiss=modal>
Close</button></div></div></div></div><script src=https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.js></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/contrib/auto-render.min.js onload=renderMathInElement(document.body)></script><script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script><script src=https://pouceheure.github.io/blog/js/setup-latex.js></script><script src=https://pouceheure.github.io/blog/js/highlight-navbar.js></script><script>highlightNavbar("https://pouceheure.github.io/blog/")</script><script src=https://pouceheure.github.io/blog/js/webmail-reveal.js></script></body></html>